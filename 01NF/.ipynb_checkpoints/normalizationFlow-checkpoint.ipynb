{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Intro \n",
    "This is practice for normalization flow.  All the resource comes from  https://gebob19.github.io/normalizing-flows/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1 R-NVP Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a single R-NVP function $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d}$, with input $\\mathbf{x} \\in \\mathbb{R}^{d}$ and output $\\mathbf{z} \\in \\mathbb{R}^{d}$.\n",
    "\n",
    "To quickly recap, in order to optimize our function $f$ to model our data distribution $p_{X}$, we want to know the forward pass $f$, and the Jacobian $\\left|\\operatorname{det}\\left(\\frac{d f}{d x}\\right)\\right|$.\n",
    "\n",
    "We then will want to know the inverse of our function $f^{-1}$ so we can transform a sampled latent-value $z \\sim p_{Z}$ to our data distribution $p_{X}$, generating new samples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Forward Pass\n",
    "first split x(1...d) into 2 pars i.e. x(1...k),x(k+1...d)\n",
    "\n",
    "R-NVPs forward pass is then the following\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\mathbf{z}_{1: k}=\\mathbf{x}_{1: k} \\\\\n",
    "\\mathbf{z}_{k+1: d}=\\mathbf{x}_{k+1: d} \\odot \\exp \\left(\\sigma\\left(\\mathbf{x}_{1: k}\\right)\\right)+\\mu\\left(\\mathbf{x}_{1: k}\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Where $\\sigma, \\mu: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{d-k}$ and are any arbitrary functions. Hence, we will choose $\\sigma$ and $\\mu$ to both be deep neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d35c46f8d1a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'"
     ]
    }
   ],
   "source": [
    "import pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self,x):\n",
    "    x1,x2=x[:,:self.k],x[:,self.k:]\n",
    "    \n",
    "    sig=self.sig_net(x1)\n",
    "    mu=self.mu_net(x1)\n",
    "    \n",
    "    z1=x1\n",
    "    z2=x2*torch.exp(sig)+mu\n",
    "    z=torch.cat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
