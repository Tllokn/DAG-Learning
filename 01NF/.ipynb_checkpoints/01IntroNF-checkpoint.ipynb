{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro \n",
    "This is practice for normalization flow.  All the resource comes from  https://gebob19.github.io/normalizing-flows/\n",
    "\n",
    "### Part1 R-NVP Flows\n",
    "\n",
    "We consider a single R-NVP function $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d}$, with input $\\mathbf{x} \\in \\mathbb{R}^{d}$ and output $\\mathbf{z} \\in \\mathbb{R}^{d}$.\n",
    "\n",
    "To quickly recap, in order to optimize our function $f$ to model our data distribution $p_{X}$, we want to know the forward pass $f$, and the Jacobian $\\left|\\operatorname{det}\\left(\\frac{d f}{d x}\\right)\\right|$.\n",
    "\n",
    "We then will want to know the inverse of our function $f^{-1}$ so we can transform a sampled latent-value $z \\sim p_{Z}$ to our data distribution $p_{X}$, generating new samples!\n",
    "\n",
    "### Key Equations\n",
    "$$\n",
    "\\log \\left(p_{X}(x)\\right)=\\log \\left(\\left|\\operatorname{det}\\left(\\frac{d f}{d x}\\right)\\right|\\right)+\\log \\left(p_{Z}(f(x))\\right)\n",
    "$$\n",
    "Using the handy dandy chain rule, we can get\n",
    "$$\n",
    "\\log \\left(p_{X}(x)\\right)=\\log \\left(\\prod_{i=1}^{n}\\left|\\operatorname{det}\\left(\\frac{d z_{i}}{d z_{i-1}}\\right)\\right|\\right)+\\log \\left(p_{Z}(f(x))\\right)\n",
    "$$\n",
    "Where $x \\triangleq z_{0}$ for conciseness.\n",
    "$$\n",
    "\\log \\left(p_{X}(x)\\right)=\\sum_{i=1}^{n} \\log \\left(\\left|\\operatorname{det}\\left(\\frac{d z_{i}}{d z_{i-1}}\\right)\\right|\\right)+\\log \\left(p_{Z}(f(x))\\right)\n",
    "$$\n",
    "\n",
    "#### 1.1 Forward Pass\n",
    "first split x(1...d) into 2 pars i.e. x(1...k),x(k+1...d)\n",
    "\n",
    "R-NVPs forward pass is then the following\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\mathbf{z}_{1: k}=\\mathbf{x}_{1: k} \\\\\n",
    "\\mathbf{z}_{k+1: d}=\\mathbf{x}_{k+1: d} \\odot \\exp \\left(\\sigma\\left(\\mathbf{x}_{1: k}\\right)\\right)+\\mu\\left(\\mathbf{x}_{1: k}\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Where $\\sigma, \\mu: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{d-k}$ and are any arbitrary functions. Hence, we will choose $\\sigma$ and $\\mu$ to both be deep neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self,x):\n",
    "    x1,x2=x[:,:self.k],x[:,self.k:]\n",
    "    \n",
    "    sig=self.sig_net(x1)\n",
    "    mu=self.mu_net(x1)\n",
    "    \n",
    "    z1=x1\n",
    "    z2=x2*torch.exp(sig)+mu\n",
    "    z=torch.cat([z1,z2],dim=-1)\n",
    "    \n",
    "    # log(p_Z(f(x)))\n",
    "    log_pz=self.p_Z.log_prob(z)\n",
    "    \n",
    "    #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Log Jacobian\n",
    "The Jacobian matrix $\\frac{d f}{d \\mathbf{x}}$ of this function will be\n",
    "$$\n",
    "\\left[\\begin{array}{cc}\n",
    "I_{d} & 0 \\\\\n",
    "\\frac{d z_{k+1: d}}{d \\mathbf{x}_{1: k}} & \\operatorname{diag}\\left(\\exp \\left[\\sigma\\left(\\mathbf{x}_{1: k}\\right)\\right]\\right)\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "The log determinant of such a Jacobian Matrix will be\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\log \\left(\\operatorname{det}\\left(\\frac{d f}{d \\mathbf{x}}\\right)\\right)=\\log \\left(\\prod_{i=1}^{d-k}\\left|\\exp \\left[\\sigma_{i}\\left(\\mathbf{x}_{1: k}\\right)\\right]\\right|\\right) \\\\\n",
    "\\log \\left(\\left|\\operatorname{det}\\left(\\frac{d f}{d \\mathbf{x}}\\right)\\right|\\right)=\\sum_{i=1}^{d-k} \\log \\left(\\exp \\left[\\sigma_{i}\\left(\\mathbf{x}_{1: k}\\right)\\right]\\right) \\\\\n",
    "\\log \\left(\\left|\\operatorname{det}\\left(\\frac{d f}{d \\mathbf{x}}\\right)\\right|\\right)=\\sum_{i=1}^{d-k} \\sigma_{i}\\left(\\mathbf{x}_{1: k}\\right)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single R-NVP calculation\n",
    "def forward(x): \n",
    "  #...\n",
    "  log_jacob = sig.sum(-1)\n",
    "  #...\n",
    "  \n",
    "  return z, log_pz, log_jacob\n",
    "\n",
    "# multiple sequential R-NVP calculation\n",
    "def forward(self, x):\n",
    "  log_jacobs = []\n",
    "  z = x\n",
    "  \n",
    "  for rvnp in self.rvnps:\n",
    "      z, log_pz, log_j = rvnp(z)\n",
    "      log_jacobs.append(log_j)\n",
    "\n",
    "  return z, log_pz, sum(log_jacobs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Inverse\n",
    "$$\n",
    "f^{-1}(\\mathbf{z})=\\mathbf{x}\n",
    "$$\n",
    "One of the benefits of R-NVPs compared to other flows is the ease of inverting $f$ into $f^{-1}$, which we formulate below:\n",
    "$$\\mathbf{x}_{1: k}=\\mathbf{z}_{1: k}$$\n",
    "$\\mathbf{x}_{k+1: d}=\\left(\\mathbf{z}_{k+1: d}-\\mu\\left(\\mathbf{x}_{1: k}\\right)\\right) \\odot \\exp \\left(-\\sigma\\left(\\mathbf{x}_{1: k}\\right)\\right)$\n",
    "$\\Leftrightarrow \\mathbf{x}_{k+1: d}=\\left(\\mathbf{z}_{k+1: d}-\\mu\\left(\\mathbf{z}_{1: k}\\right)\\right) \\odot \\exp \\left(-\\sigma\\left(\\mathbf{z}_{1: k}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse(self, z):\n",
    "  z1, z2 = z[:, :self.k], z[:, self.k:] \n",
    "  \n",
    "  sig = self.sig_net(z1)\n",
    "  mu = self.mu_net(z1)\n",
    "  \n",
    "  x1 = z1\n",
    "  x2 = (z2 - mu) * torch.exp(-sig)\n",
    "  \n",
    "  x = torch.cat([x1, x2], dim=-1)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8775ceda725a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "for _ in range(epochs):\n",
    "  optim.zero_grad()\n",
    "  \n",
    "  # forward pass\n",
    "  X = get_batch(data)\n",
    "  z, log_pz, log_jacob = model(X)\n",
    "  \n",
    "  # maximize p_X(x) == minimize -p_X(x)\n",
    "  loss = -(log_jacob + log_pz).mean()\n",
    "  losses.append(loss)\n",
    "\n",
    "  # backpropigate loss\n",
    "  loss.backward()\n",
    "  optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Generate data from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_Z - gaussian\n",
    "mu, cov = torch.zeros(2), torch.eye(2)\n",
    "p_Z = MultivariateNormal(mu, cov)\n",
    "\n",
    "# sample 3000 points (z ~ p_Z)\n",
    "z = p_Z.rsample(sample_shape=(3000,))\n",
    "\n",
    "# invert f^-1(z) = x\n",
    "x_gen = model.inverse(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python37064bit3f56889c3a0843499be1aa2b16c5c20e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
